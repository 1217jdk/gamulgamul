{
  "metadata": {
    "name": "goods_price_table_new",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.conf\n\nSPARK_HOME  /usr/local/spark\nPYSPARK_PYTHON /usr/bin/python3\nspark.pyspark.python  /usr/bin/python3\n\n# set driver memory to 8g\nspark.driver.memory 6g\n\n# set executor number to be 3\nspark.executor.instances  4\n\n# set executor memory 4g\nspark.executor.memory  2g"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nimport pandas as pd\nimport numpy as np\nimport datetime\nfrom tqdm import tqdm\nimport os\nimport sys\nimport urllib.request\nimport json, pprint\nimport pickle\nimport time"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nwith open(\u0027/DB_data/goodsName_goodsId.pickle\u0027, \u0027rb\u0027) as f:\n  old_goodsName_goodsId \u003d pickle.load(f)\nold_goodsName_goodsId"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# 상품명, 용량이 바뀌지 않아서 이전에 쌓아놓은 daily goods price를 사용하는 경우"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nold_goods_price_daily \u003d pd.read_csv(\u0027/DB_data/goods_daily_price_table.csv\u0027)\nold_goods_price_daily"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ngoods_table \u003d pd.read_csv(\u0027/DB_data/goods_table_new.csv\u0027)\ngoods_table"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nold_goods_price_daily.loc[(old_goods_price_daily[\u0027goods_id\u0027]\u003d\u003d0) \u0026 (old_goods_price_daily[\u0027business\u0027]\u003d\u003d\u0027m\u0027)]"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ngoods_table[goods_table[\u0027goods_id\u0027]\u003d\u003d0]"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ngoods_price_not_change \u003d []\ngoods_price_change \u003d []\nfor i in range(len(goods_table)):\n  goods \u003d goods_table.loc[i]\n  try:\n    old_goods_id \u003d old_goodsName_goodsId[goods[\u0027name\u0027]]\n  except:\n    goods_price_change.append(goods_table[goods_table[\u0027goods_id\u0027]\u003d\u003dgoods[\u0027goods_id\u0027]])\n    continue\n  old_goods \u003d old_goods_price_daily.loc[(old_goods_price_daily[\u0027goods_id\u0027]\u003d\u003dold_goods_id) \u0026 (old_goods_price_daily[\u0027business\u0027]\u003d\u003d\u0027m\u0027)]\n  old_goods[\u0027goods_id\u0027] \u003d goods[\u0027goods_id\u0027]\n  goods_price_not_change.append(old_goods)"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ngoods_price_daily_not_change \u003d pd.concat(goods_price_not_change, ignore_index\u003dTrue)\ngoods_price_daily_not_change"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# 상품명이나 용량이 바뀌어서 naver api로 받아온 새로운 값으로 1년을 채워줘야 하는 경우"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ngoods_price_change_df \u003d pd.concat(goods_price_change, ignore_index\u003dTrue)\ngoods_price_change_df"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ngoods_search_option \u003d pd.read_csv(\u0027/DB_data/tp_goods_search_option.csv\u0027)\ngoods_search_option"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nproduct_table \u003d pd.read_csv(\u0027/DB_data/product_table.csv\u0027)\nproduct_table_unit \u003d product_table[[\u0027product_id\u0027, \u0027unit\u0027]]\nproduct_table_unit"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ngoods_search_option.loc[goods_search_option[\u0027goods_id\u0027]\u003d\u003d0]"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ngoods_search_option.loc[goods_search_option[\u0027goods_id\u0027]\u003d\u003d0][\u0027sort option\u0027].values[0]"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n### api 키\nclient_id \u003d \"a8DDB5LJUVwh8_MWPtSj\"\nclient_secret \u003d \"hKlWjoMufh\"\n\n\ndef get_goods_info(goods_id):\n    row \u003d goods_search_option.loc[goods_search_option[\u0027goods_id\u0027]\u003d\u003dgoods_id]\n    mall_name \u003d row[\u0027mall_name\u0027].values[0]\n    search_name \u003d row[\u0027search_name\u0027].values[0]\n    sort_option \u003d \u0027sim\u0027 if pd.isna(row[\u0027sort option\u0027].values[0]) else \u0027asc\u0027\n    divide \u003d row[\u0027divide\u0027].values[0]\n\n    encText \u003d urllib.parse.quote(f\u0027{mall_name} {search_name}\u0027)\n    url \u003d f\"https://openapi.naver.com/v1/search/shop.json?exclude\u003dused:rental:cbshop\u0026start\u003d1\u0026display\u003d20\u0026sort\u003d{sort_option}\u0026query\u003d\" + encText \n    request \u003d urllib.request.Request(url)\n    request.add_header(\"X-Naver-Client-Id\",client_id)\n    request.add_header(\"X-Naver-Client-Secret\",client_secret)\n    time.sleep(0.01)\n    response \u003d urllib.request.urlopen(request)\n    rescode \u003d response.getcode()\n\n    if(rescode\u003d\u003d200):\n        response_body \u003d response.read()\n        res \u003d json.loads(response_body.decode(\u0027utf-8\u0027))\n    else:\n        print(\"Error Code:\" + rescode)\n        return\n    \n    goods_info \u003d dict()\n    goods_info[\u0027goods_id\u0027] \u003d goods_id\n    if not res[\u0027items\u0027]:\n        return {\u0027goods_id\u0027: goods_id, \u0027unit_price\u0027: None, \u0027price\u0027: None, \u0027research_date\u0027: datetime.datetime.now().strftime(\u0027%Y-%m-%d\u0027), \u0027business\u0027: \u0027m\u0027}\n   \n    for i in range(res[\u0027display\u0027]):\n        api_mall_name \u003d res[\u0027items\u0027][i].get(\u0027mallName\u0027)\n        if api_mall_name \u003d\u003d \u0027홈플러스\u0027 or api_mall_name \u003d\u003d \u0027homeplus\u0027:\n            api_mall_name \u003d \u0027Homeplus\u0027\n        \n        if mall_name \u003d\u003d api_mall_name:\n            if id \u003c 20000:\n                price \u003d int(res[\u0027items\u0027][i].get(\u0027lprice\u0027))\n                if not pd.isna(divide):\n                    price //\u003d int(divide)\n                unit_price \u003d price * int(product_table_unit.loc[product_table_unit[\u0027product_id\u0027]\u003d\u003drow[\u0027product_id\u0027].values[0]][\u0027unit\u0027]) / int(row[\u0027amount\u0027].values[0])\n                goods_info[\u0027unit_price\u0027] \u003d round(unit_price, 2)\n                # goods_info[\u0027unit_price\u0027] \u003d unit_price\n                goods_info[\u0027price\u0027] \u003d price\n            else:\n                goods_info[\u0027unit_price\u0027] \u003d int(res[\u0027items\u0027][i].get(\u0027lprice\u0027)) if id !\u003d 20018 else int(res[\u0027items\u0027][i].get(\u0027lprice\u0027))/20   # 쌀이라면 나누기 20해줘야함\n                goods_info[\u0027price\u0027] \u003d int(res[\u0027items\u0027][i].get(\u0027lprice\u0027))\n            \n            goods_info[\u0027research_date\u0027] \u003d datetime.datetime.now().strftime(\u0027%Y-%m-%d\u0027)\n            goods_info[\u0027business\u0027] \u003d \u0027m\u0027\n            return goods_info\n    else:\n        return {\u0027goods_id\u0027: goods_id, \u0027unit_price\u0027: None, \u0027price\u0027: None, \u0027research_date\u0027: datetime.datetime.now().strftime(\u0027%Y-%m-%d\u0027), \u0027business\u0027: \u0027m\u0027}\n    \n    return {\u0027goods_id\u0027: goods_id, \u0027unit_price\u0027: None, \u0027price\u0027: None, \u0027research_date\u0027: datetime.datetime.now().strftime(\u0027%Y-%m-%d\u0027), \u0027business\u0027: \u0027m\u0027}"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ngoods_price_change_df[\u0027goods_id\u0027]"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nchange_goods_info \u003d []\nfor id in goods_price_change_df[\u0027goods_id\u0027]:\n    change_goods_info.append(get_goods_info(id))"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nchange_goods_info"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nchange_goods_price_m \u003d pd.DataFrame(change_goods_info)\nchange_goods_price_m"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nchange_goods_price_m[change_goods_price_m[\u0027goods_id\u0027]\u003d\u003d394]"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# 코드 줄인 버전\nimport datetime\nimport numpy as np\nimport pandas as pd\n\n\n\n# row 형식으로 된, 일데이터 주기가 아닌 데이터를, 일데이터 형식으로 바꾸기\ndef inserting_date(start_date, end_date, input_df, business): # Pandas DF를 이용\n\n    #---------------------------------------------------------------------------------------------------------------------------------#\n    #                 1. 초기작업 : 결과 df 생성, 초기값들 설정\n    #---------------------------------------------------------------------------------------------------------------------------------#\n\n    \n    # 초기 설정 값\n    start_goods_id \u003d min((input_df[\u0027goods_id\u0027]))\n    end_goods_id \u003d max((input_df[\u0027goods_id\u0027]))\n    cur_goods_id \u003d start_goods_id - 1\n    cur_research_date \u003d start_date\n\n    \n    # goods_id, research_date 기준으로 sorting 시키기\n    input_df \u003d input_df.reset_index(drop\u003dTrue)\n    input_df \u003d input_df.sort_values([\u0027goods_id\u0027,\u0027research_date\u0027])\n    # 추후에 삭제할 행\n    input_df.loc[len(input_df)] \u003d [end_goods_id+1, 0, 0 ,start_date.strftime(\"%Y-%m-%d\"), business]\n\n    # result\n    result_df \u003d pd.DataFrame(columns\u003dinput_df.columns)\n    inserting_length \u003d 0\n    \n    \n    \n    input_df_length \u003d 0\n    # input_df의 끝까지 가지 않은 경우, 계속하기\n    ## 1. goods_id 가 달라지는 경우, research_date가 일치하면, input_df_length를 1 늘리기\n    while input_df_length \u003c len(input_df): # dummy row까지 쭉 진행,\n    \n        next_goods_id, next_unit_price, next_price, next_research_date, next_business \u003d input_df.loc[input_df_length]   # input_df의 다음(목표) row\n        # inserting_research_date \u003d cur_research_date                                                    # input_df의 현재 row\n    \n        \n        # goods_id가 같은 경우\n        if next_goods_id \u003d\u003d cur_goods_id:\n            ## input_df의 next row에 도달한 경우, 도달한 값 넣어주기\n    \n            ## 현재 inserting 날짜가, next_research_date에 도달하기 전까지 , 계속 row 삽입하기\n            while next_research_date !\u003d inserting_research_date.strftime(\"%Y-%m-%d\"):\n                # print(inserting_research_date.strftime(\"%Y-%m-%d\"), \u0027--------------------\u0027)\n                result_df.loc[inserting_length] \u003d cur_goods_id, cur_unit_price, cur_price, inserting_research_date.strftime(\"%Y-%m-%d\"),  cur_business\n                \n                \n                cur_research_date \u003d inserting_research_date                        # cur_research_date 업데이트\n                \n\n                inserting_research_date +\u003d  datetime.timedelta(days\u003d1) # inserting할 날짜 늘리기\n                inserting_length +\u003d 1   # result_df 길이 추가\n                \n            cur_goods_id \u003d next_goods_id                                       # cur_goods_id 업데이트\n            cur_unit_price \u003d next_unit_price\n            cur_price \u003d next_price                                             # cur_price 업데이트\n            cur_business \u003d next_business                                       # cur_business 업데이트\n            input_df_length +\u003d 1\n                \n    \n           \n    \n    \n        # 새로운 goods_id를 넣는 경우, 새로 진행\n        elif next_goods_id !\u003d cur_goods_id:\n            # print(cur_goods_id)\n\n            ## end_date 까지 row 채워주기\n            if cur_goods_id !\u003d start_goods_id - 1:  # 처음 시작하는 경우가 아니라면,\n                while end_date + datetime.timedelta(days\u003d1) \u003e inserting_research_date:\n                    result_df.loc[inserting_length] \u003d cur_goods_id, cur_unit_price,cur_price, inserting_research_date.strftime(\"%Y-%m-%d\"),  cur_business\n                    inserting_research_date +\u003d  datetime.timedelta(days\u003d1) # inserting할 날짜 늘리기\n                    inserting_length +\u003d 1   # result_df 길이 추가\n            \n            ## input_df의 마지막까지 진행됐다면 while문 break해서 끝내기\n            if input_df_length \u003d\u003d len(input_df) - 1:\n                print(\u0027here\u0027)\n                break\n            \n            ## inserting_research_date 초기화\n            inserting_research_date \u003d start_date\n            \n            ## 만약 첫 row가 start_date일이 아니면, 채워주기\n            inserting_unit_price \u003d next_unit_price\n            inserting_price \u003d next_price  # next_price의 가격을 앞에 채워주기\n            ### 다음 row의 날짜에 도달하기 전까지 계속 추가해 주기\n            while next_research_date !\u003d inserting_research_date.strftime(\"%Y-%m-%d\"):\n                result_df.loc[inserting_length] \u003d next_goods_id, next_unit_price, inserting_price, inserting_research_date.strftime(\"%Y-%m-%d\"), next_business\n\n                cur_research_date \u003d inserting_research_date                        # cur_research_date 업데이트\n                \n                \n                inserting_research_date +\u003d  datetime.timedelta(days\u003d1) # inserting할 날짜 늘리기\n                inserting_length +\u003d 1   # result_df 길이 추가\n            \n            input_df_length +\u003d 1    # input_df 길이 추가\n            cur_goods_id \u003d next_goods_id                                       # cur_goods_id 업데이트\n            cur_unit_price \u003d next_unit_price\n            cur_price \u003d next_price                                             # cur_price 업데이트\n            \n            cur_business \u003d next_business                                       # cur_business 업데이트\n            \n    return result_df"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nstart_date \u003d datetime.date(2021,9,7)\nend_date \u003d datetime.date(2022,9,30)\ninput_df1 \u003d change_goods_price_m[(change_goods_price_m[\u0027goods_id\u0027]\u003e\u003d0) \u0026 (change_goods_price_m[\u0027goods_id\u0027]\u003c200) \u0026  (change_goods_price_m[\u0027business\u0027]\u003d\u003d\u0027m\u0027)]\ninput_df2 \u003d change_goods_price_m[(change_goods_price_m[\u0027goods_id\u0027]\u003e\u003d200) \u0026 (change_goods_price_m[\u0027business\u0027]\u003d\u003d\u0027m\u0027)]"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nresult_df1 \u003d inserting_date(start_date, end_date, input_df1,\u0027m\u0027)\nresult_df2 \u003d inserting_date(start_date, end_date, input_df2,\u0027m\u0027)"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nchange_goods_price \u003d pd.concat([result_df1, result_df2], ignore_index\u003dTrue)\nchange_goods_price"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nchange_goods_price_sdf \u003d spark.createDataFrame(change_goods_price)\nchange_goods_price_sdf[(change_goods_price_sdf[\u0027research_date\u0027]\u003d\u003d\u00272021-09-07\u0027)|(change_goods_price_sdf[\u0027research_date\u0027]\u003d\u003d\u00272022-09-30\u0027)|(change_goods_price_sdf[\u0027research_date\u0027]\u003d\u003d\u00272022-03-15\u0027)].show(500,truncate\u003dFalse)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# 대형마트 goods_price table 상품명 변경 버전(goods_price_table_new)"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nchange_goods_price \u003d pd.concat([goods_price_daily_not_change, change_goods_price], ignore_index\u003dTrue)\nchange_goods_price.sort_values(by\u003d[\u0027goods_id\u0027, \u0027research_date\u0027], inplace\u003dTrue)\nchange_goods_price.reset_index(drop\u003dTrue, inplace\u003dTrue)\nchange_goods_price"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nchange_goods_price.to_csv(\u0027/DB_data/goods_price_table_new.csv\u0027, index\u003dFalse)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# 온라인 goods_price_table 채우기\n## 일단 참가격 상품만(20000번 미만)"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ngoods_table_tp \u003d goods_table[goods_table[\u0027goods_id\u0027]\u003c20000]\ngoods_table_tp"
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ngoods_table.loc[goods_table[\u0027goods_id\u0027]\u003d\u003d0][\u0027name\u0027].values[0]"
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n### api 키\nclient_id \u003d \"a8DDB5LJUVwh8_MWPtSj\"\nclient_secret \u003d \"hKlWjoMufh\"\n\n\ndef get_online_goods_info(goods_id):\n    row \u003d goods_search_option.loc[goods_search_option[\u0027goods_id\u0027]\u003d\u003dgoods_id]\n    if goods_id in [49, 53, 231, 232, 347, 363, 406]:\n        search_name \u003d goods_table.loc[goods_table[\u0027goods_id\u0027]\u003d\u003dgoods_id][\u0027name\u0027].values[0]\n    else:\n        search_name \u003d row[\u0027search_name\u0027].values[0]\n    if search_name \u003d\u003d \u0027주류\u0027:\n        return {\u0027goods_id\u0027: goods_id, \u0027unit_price\u0027: None, \u0027price\u0027: None, \u0027research_date\u0027: datetime.datetime.now().strftime(\u0027%Y-%m-%d\u0027), \u0027business\u0027: \u0027o\u0027}\n    divide \u003d row[\u0027divide\u0027].values[0]\n\n    encText \u003d urllib.parse.quote(f\u0027{search_name}\u0027)\n    url \u003d \"https://openapi.naver.com/v1/search/shop.json?exclude\u003dused:rental:cbshop\u0026start\u003d1\u0026display\u003d20\u0026sort\u003dsim\u0026query\u003d\" + encText \n    request \u003d urllib.request.Request(url)\n    request.add_header(\"X-Naver-Client-Id\",client_id)\n    request.add_header(\"X-Naver-Client-Secret\",client_secret)\n    time.sleep(0.01)\n    response \u003d urllib.request.urlopen(request)\n    rescode \u003d response.getcode()\n\n    if(rescode\u003d\u003d200):\n        response_body \u003d response.read()\n        res \u003d json.loads(response_body.decode(\u0027utf-8\u0027))\n    else:\n        print(\"Error Code:\" + rescode)\n        return\n    \n    goods_info \u003d dict()\n    goods_info[\u0027goods_id\u0027] \u003d goods_id\n    if not res[\u0027items\u0027]:\n        return {\u0027goods_id\u0027: goods_id, \u0027unit_price\u0027: None, \u0027price\u0027: None, \u0027research_date\u0027: datetime.datetime.now().strftime(\u0027%Y-%m-%d\u0027), \u0027business\u0027: \u0027o\u0027}\n    \n    cheap \u003d sorted(res[\u0027items\u0027], key\u003d lambda x: int(x[\u0027lprice\u0027]))[0]\n    \n    if id \u003c 20000:\n        price \u003d int(cheap.get(\u0027lprice\u0027))\n        if not pd.isna(divide):\n            price //\u003d int(divide)\n        unit_price \u003d price * int(product_table_unit.loc[product_table_unit[\u0027product_id\u0027]\u003d\u003drow[\u0027product_id\u0027].values[0]][\u0027unit\u0027]) / int(row[\u0027amount\u0027].values[0])\n        goods_info[\u0027unit_price\u0027] \u003d round(unit_price, 2)\n        # goods_info[\u0027unit_price\u0027] \u003d unit_price\n        goods_info[\u0027price\u0027] \u003d price\n    else:\n        goods_info[\u0027unit_price\u0027] \u003d int(cheap.get(\u0027lprice\u0027)) if id !\u003d 20018 else int(cheap.get(\u0027lprice\u0027))/20   # 쌀이라면 나누기 20해줘야함\n        goods_info[\u0027price\u0027] \u003d int(cheap.get(\u0027lprice\u0027))\n    \n    goods_info[\u0027research_date\u0027] \u003d datetime.datetime.now().strftime(\u0027%Y-%m-%d\u0027)\n    goods_info[\u0027business\u0027] \u003d \u0027o\u0027\n    \n    return goods_info\n    "
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ntp_online_goods_info \u003d []\nfor id in goods_table_tp[\u0027goods_id\u0027]:\n    tp_online_goods_info.append(get_online_goods_info(id))"
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ntp_online_goods_info"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "주류 아닌데 None 뜨는 goods_id\n- 49, 53, 231, 232, 347, 363, 406\n    - 이 중에서 231, 232, 406은 이름 또 변경\n    - 그리고 search_name이 아닌 name으로 검색\n\n주류\n- 254, 255, 256, 257, 258, 259, 269, 270, 271, 292, 350, 351, 352, 353, 354, 355, 361 "
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ngoods_table.loc[goods_table[\u0027goods_id\u0027]\u003d\u003d231, \u0027name\u0027] \u003d \u0027떠먹는불가리스 딸기(4개묶음)\u0027\ngoods_table.loc[goods_table[\u0027goods_id\u0027]\u003d\u003d232, \u0027name\u0027] \u003d \u0027매일 바이오 플레인 스위트(85g4)\u0027\ngoods_table.loc[goods_table[\u0027goods_id\u0027]\u003d\u003d406, \u0027name\u0027] \u003d \u0027알찬란(대란 30개)\u0027\ngoods_table.to_csv(\u0027/DB_data/goods_table_new.csv\u0027, index\u003dFalse)"
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ngoodsName_goodsId_new \u003d dict(zip(goods_table.name, goods_table.goods_id))\ngoodsName_goodsId_new"
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nwith open(\u0027/DB_data/goodsName_goodsId_new.pickle\u0027, \u0027wb\u0027) as f:\n    pickle.dump(goodsName_goodsId_new, f)"
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nchange_goods_price_o \u003d pd.DataFrame(tp_online_goods_info)\nchange_goods_price_o"
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ninput_df2"
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nstart_date \u003d datetime.date(2021,9,7)\nend_date \u003d datetime.date(2022,9,30)\ninput_df1 \u003d change_goods_price_o[(change_goods_price_o[\u0027goods_id\u0027]\u003e\u003d0) \u0026 (change_goods_price_o[\u0027goods_id\u0027]\u003c100) \u0026  (change_goods_price_o[\u0027business\u0027]\u003d\u003d\u0027o\u0027)]\ninput_df2 \u003d change_goods_price_o[(change_goods_price_o[\u0027goods_id\u0027]\u003e\u003d100) \u0026 (change_goods_price_o[\u0027goods_id\u0027]\u003c200) \u0026 (change_goods_price_o[\u0027business\u0027]\u003d\u003d\u0027o\u0027)]\ninput_df3 \u003d change_goods_price_o[(change_goods_price_o[\u0027goods_id\u0027]\u003e\u003d200) \u0026 (change_goods_price_o[\u0027goods_id\u0027]\u003c300) \u0026 (change_goods_price_o[\u0027business\u0027]\u003d\u003d\u0027o\u0027)]\ninput_df4 \u003d change_goods_price_o[(change_goods_price_o[\u0027goods_id\u0027]\u003e\u003d300) \u0026 (change_goods_price_o[\u0027business\u0027]\u003d\u003d\u0027o\u0027)]\n\nresult_df1 \u003d inserting_date(start_date, end_date, input_df1, \u0027o\u0027)\nresult_df2 \u003d inserting_date(start_date, end_date, input_df2, \u0027o\u0027)\nresult_df3 \u003d inserting_date(start_date, end_date, input_df3, \u0027o\u0027)\nresult_df4 \u003d inserting_date(start_date, end_date, input_df4, \u0027o\u0027)"
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ntp_online_goods_price \u003d pd.concat([result_df1, result_df2, result_df3, result_df4], ignore_index\u003dTrue)\ntp_online_goods_price.sort_values(by\u003d[\u0027goods_id\u0027, \u0027research_date\u0027], inplace\u003dTrue)\ntp_online_goods_price.reset_index(drop\u003dTrue, inplace\u003dTrue)\ntp_online_goods_price"
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ntp_online_goods_price.to_csv(\u0027/DB_data/goods_price_table_new_tprice.csv\u0027, index\u003dFalse)"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    }
  ]
}