{
  "metadata": {
    "name": "recommendation",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.conf\n\nSPARK_HOME  /usr/local/spark\nPYSPARK_PYTHON /usr/bin/python3\nspark.pyspark.python  /usr/bin/python3\n\n\n# set driver memory to 8g\nspark.driver.memory 8g\n\n# set executor number to be 3\nspark.executor.instances  3\n\n# set executor memory 4g\nspark.executor.memory  2g\n\n# Any other spark properties can be set here. Here\u0027s avaliable spark configruation you can set. (http://spark.apache.org/docs/latest/configuration.html)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# 즐겨찾기 상품 추천 시스템"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%MySQL\nshow tables;"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%MySQL\nselect * from users"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### favorite_goods 테이블 기반으로 추천 해줘야 함"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%MySQL\nselect * from favorite_goods"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1. 상품을 누가 즐겨찾기에 넣었는지 안 넣었는지 랜덤으로 값 넣어주기"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nusers_sdf \u003d spark.read.format(\"jdbc\")\\\n    .option(\"url\", \"jdbc:mysql://j7d108.p.ssafy.io:3306/gamulgamul_test3?useSSL\u003dfalse\u0026characterEncoding\u003dUTF-8\u0026useUnicode\u003dtrue\u0026allowPublicKeyRetrieval\u003dtrue\u0026serverTimezone\u003dAsia/Seoul\") \\\n    .option(\"driver\", \"com.mysql.cj.jdbc.Driver\")\\\n    .option(\"query\", \"select user_id from users\")\\\n    .option(\"user\", \"ji\")\\\n    .option(\"password\", \"cvgkbhs234r#8tdx\").load()\nusers_sdf \u003d users_sdf.toPandas()\nusers_id \u003d users_sdf[\u0027user_id\u0027]"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ngoods_table \u003d spark.read.csv(\u0027/DB_data/goods_table.csv\u0027, header\u003dTrue, inferSchema\u003dTrue)\ngoods_table.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nimport numpy as np\nnp.random.choice(users_id, 6)"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nimport pickle\nwith open(\u0027/DB_data/goodsName_goodsId.pickle\u0027, \u0027rb\u0027) as f:\n    goodsId \u003d pickle.load(f)\ngoodsId.values()"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ngoods_id \u003d []\nfor v in goodsId.values():\n    for _ in range(14):\n        goods_id.append(v)\ngoods_id"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nimport numpy as np\nuser_id \u003d np.array([])\nfor i in range(len(goodsId.values())):\n    user_id \u003d np.append(user_id, users_id)\nuser_id \u003d user_id.astype(int)\nuser_id"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nscore \u003d []\nfor i in range(len(goodsId.values())):\n    for _ in range(14):\n        score.append(np.random.randint(2))"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nprint(len(goods_id))\nprint(len(user_id))\nprint(len(score))"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nimport pandas as pd\ndf \u003d pd.DataFrame({\u0027goods_id\u0027: goods_id, \u0027user_id\u0027: user_id, \u0027score\u0027: score})\ndf"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# spark.conf.set(\"spark.sql.execution.arrow.enabled\",\"true\")\nsdf\u003dspark.createDataFrame(df) \n# sdf.printSchema()\nsdf.show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### favorite_goods 기반으로 sdf 만들기"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfav_sdf \u003d spark.read.format(\"jdbc\")\\\n    .option(\"url\", \"jdbc:mysql://j7d108.p.ssafy.io:3306/gamulgamul_test3?useSSL\u003dfalse\u0026characterEncoding\u003dUTF-8\u0026useUnicode\u003dtrue\u0026allowPublicKeyRetrieval\u003dtrue\u0026serverTimezone\u003dAsia/Seoul\") \\\n    .option(\"driver\", \"com.mysql.cj.jdbc.Driver\")\\\n    .option(\"query\", \"select goods_id, user_id, reg_date from favorite_goods\")\\\n    .option(\"user\", \"ji\")\\\n    .option(\"password\", \"cvgkbhs234r#8tdx\").load()\nfav_sdf.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark.sql.functions import col\n\nfav_sdf.select(\n    datediff(current_date(), col(\u0027reg_date\u0027))\n    ).show()"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nimport math\n\ndef sigmoid(x):\n  return 1 / (1 + math.exp(-x))\n \nsigmoid \u003d udf(sigmoid)"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfav_sdf \u003d fav_sdf.select(\n    \u0027goods_id\u0027,\n    \u0027user_id\u0027,\n    sigmoid(datediff(current_date(), col(\u0027reg_date\u0027))).cast(\u0027float\u0027).alias(\u0027score\u0027)\n    )\nfav_sdf.show()"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2. 추천 모델 생성"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# 추천 모델 생성\nfrom pyspark.ml.recommendation import ALS\n\ntrain, test \u003d fav_sdf.randomSplit([0.75, 0.25], seed\u003d1)\n\nfav_rec \u003d ALS(maxIter\u003d10,\n         regParam\u003d0.01,\n         userCol\u003d\u0027user_id\u0027,\n         itemCol\u003d\u0027goods_id\u0027,\n         ratingCol\u003d\u0027score\u0027, \n         nonnegative\u003dTrue,\n         coldStartStrategy\u003d\u0027drop\u0027)\n\n# ALS모델 학습 -\u003e dataframe을 넣어주기\nfav_rec_model \u003d fav_rec.fit(train)\n\n# transform을 이용해 예측 -\u003e dataframe을 넣어주기\nfav_pred_ratings \u003d fav_rec_model.transform(test)"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# 추천모델 평가\n# Get metric for training\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\nevaluator \u003d RegressionEvaluator(labelCol\u003d\u0027score\u0027,\n                               predictionCol\u003d\u0027prediction\u0027,\n                               metricName\u003d\u0027rmse\u0027)\n# evaluate 메소드에 예측값 담겨있는 dataframe 넣어주기\nrmse \u003d evaluator.evaluate(fav_pred_ratings)\n\nmae_eval \u003d RegressionEvaluator(labelCol\u003d\u0027score\u0027,\n                              predictionCol\u003d\u0027prediction\u0027,\n                              metricName\u003d\u0027mae\u0027)\nmae \u003d mae_eval.evaluate(fav_pred_ratings)\n\nprint(\"RMSE:\", rmse)\nprint(\"MAE:\", mae)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3. user_id에게 즐겨찾기에 담을 수 있는 상품 n개 추천 함수"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# Pyspark Library #\n# SQL\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import SQLContext\nfrom pyspark.sql.functions import mean, col, split, regexp_extract, when, lit\n# # ML\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler, IndexToString\nfrom pyspark.ml.feature import QuantileDiscretizer\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.ml.recommendation import ALS\n\n\ndef top_fav(user_id, n):\n    \"\"\"\n    특정 user_id가 좋아할 만한 n개의 상품 추천해주는 함수\n    \"\"\"\n \n    # 특정 user_id가 즐겨찾기에 담은 상품의 상품id로 새로운 데이터프레임 생성\n    fav_goods \u003d fav_sdf.filter((fav_sdf[\u0027user_id\u0027] \u003d\u003d user_id)).select(\u0027goods_id\u0027)\n\n    # 특정 user_id가 즐겨찾기에 담은 상품들을 \u0027tmp\u0027라는 데이터프레임으로 alias시키기\n    tmp \u003d fav_goods.alias(\u0027tmp\u0027)\n    tmp \u003d tmp.withColumnRenamed(\u0027goods_id\u0027, \u0027tmp_goods_id\u0027)\n    # tmp.show()\n\n    # sdf 기준으로 tmp 조인시켜서 user_id가 즐겨찾기에 담지 않은 상품 파악 가능\n    fav_total_goods \u003d fav_sdf.join(tmp, fav_sdf[\u0027goods_id\u0027] \u003d\u003d tmp[\u0027tmp_goods_id\u0027],how\u003d\u0027left\u0027)\n    # fav_total_goods.show()\n\n    # tmp 데이터프레임의 goods_id 결측치를 갖고 있는 행의 sdf.goods_id 뽑아냄으로써 user_id가 아직 즐겨찾기에 담지 않은 상품 추출\n    fav_remain_goods \u003d fav_total_goods\\\n                       .where(col(\u0027tmp_goods_id\u0027).isNull())\\\n                       .select(\u0027goods_id\u0027).distinct()\n    # fav_remain_goods.show()\n\n    # fav_remain_goods 데이터프레임에 특정 user_id값을 동일하게 새로운 변수로 추가해주기\n    fav_remain_goods \u003d fav_remain_goods.withColumn(\u0027user_id\u0027, lit(int(user_id)))\n    # fav_remain_goods.show()\n\n    # 위에서 만든 ALS 모델을 사용하여 추천 상품 예측 후 n개 만큼 view -\u003e \n    recommender \u003d fav_rec_model.transform(fav_remain_goods)\\\n                           .orderBy(\u0027prediction\u0027, ascending\u003dFalse)\\\n                           .limit(n)\n                           \n    goods_table \u003d spark.read.csv(\u0027/DB_data/goods_table.csv\u0027, header\u003dTrue, inferSchema\u003dTrue)\n    goods_table \u003d goods_table.withColumnRenamed(\u0027goods_id\u0027, \u0027idx\u0027)\n    \n    final_recommendations \u003d goods_table.join(recommender, goods_table[\u0027idx\u0027]\u003d\u003drecommender[\u0027goods_id\u0027], how\u003d\u0027inner\u0027)\n    # final_recommendations.show()\n\n    return final_recommendations.select([\u0027goods_id\u0027, \u0027product_id\u0027, \u0027name\u0027, \u0027img\u0027, \u0027measure\u0027, \u0027cheap_url\u0027, \u0027prediction\u0027]).orderBy(\u0027prediction\u0027, ascending\u003dFalse).show(n, truncate\u003dFalse)\n\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 4. 사용자에게 추천해주기!"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ntop_fav(129242, 5)"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ntop_fav(131591, 5)"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ntop_fav(131595, 5)"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%python\n"
    }
  ]
}