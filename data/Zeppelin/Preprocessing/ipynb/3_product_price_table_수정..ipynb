{
  "metadata": {
    "name": "3_product_price_table_수정",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.conf\n\nSPARK_HOME  /usr/local/spark\nPYSPARK_PYTHON /usr/bin/python3\nspark.pyspark.python  /usr/bin/python3\n\n# set driver memory to 8g\nspark.driver.memory 8g\n\n# set executor number to be 3\nspark.executor.instances  3\n\n# set executor memory 4g\nspark.executor.memory  2g\n\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# 1. [daily] product_daily_price_table 생성하고, price(품목전체평균가격) 넣기\n+ goods_price -\u003e product_price\n+ input columns\n    + goods_id, unit_price, price, research-date, business \n+ 결과 columns\n    + product_id, unit_price, price, research_date, date_type, business "
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# product_price table 만들기\n# 10월 2일 수정\n\n# 1. goods_daily_price_pdf load\nimport pandas as pd\n\ngoods_daily_price_pdf \u003d pd.read_csv(\u0027/DB_data/20221002_goods_price_table.csv\u0027)\ngoods_daily_price_pdf\n\n# 2. product_id, date_type column 넣기\n## 2-1.  { goods_id : product_id } 를 이용해서 product_id 넣기\nimport pickle\nwith open(\u0027/DB_data/20221001_goodsId_productId_.pickle\u0027, \u0027rb\u0027) as fr:\n    goodsId_productId \u003d pickle.load(fr)\n    \n## 가독성 위해, comprehension 안 쓰고, for문으로 column 데이터 생성\nproduct_id_lst \u003d []\nfor i in range(len(goods_daily_price_pdf)):\n    goods_id \u003d goods_daily_price_pdf.loc[i,\u0027goods_id\u0027]\n    product_id \u003d goodsId_productId[goods_id]\n    product_id_lst.append(product_id)\n    \n    \n## 2-2. column 삽입\ngoods_daily_price_pdf.insert(0, \u0027product_id\u0027, product_id_lst)\ngoods_daily_price_pdf.insert(1, \u0027date_type\u0027, \u0027d\u0027)\ngoods_daily_price_pdf\n\n\n# 3. product_price 계산하기\n## 3-1 groupby로 값들 생성\nproduct_daily_price_pdf \u003d goods_daily_price_pdf.groupby([\u0027product_id\u0027,\u0027research_date\u0027,\u0027business\u0027,\u0027date_type\u0027]).mean([\u0027price\u0027,\u0027unit_price\u0027])\n\n## 3-2 groupby 풀고,  column 순서 맞추기\nproduct_daily_price_pdf \u003d product_daily_price_pdf.drop([\u0027goods_id\u0027], axis\u003d1)\nproduct_daily_price_pdf \u003d product_daily_price_pdf.reset_index()\nproduct_daily_price_pdf \u003d product_daily_price_pdf[[\u0027product_id\u0027,\u0027unit_price\u0027,\u0027price\u0027,\u0027research_date\u0027,\u0027date_type\u0027,\u0027business\u0027]]\n\n\n# 4. Ubuntu에 저장\n# product_daily_price_pdf.to_csv(\u0027/DB_data/product_daily_price_table.csv\u0027, index\u003dFalse, header\u003dTrue)\nproduct_daily_price_pdf\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# 2.  [month] product_month_price_table 수정\n+ price column -\u003e unit_price 로 column 명 바꾸기\n+ price(품목전체평균가격) column 넣기\n+ 결과 column\n    + product_id, unit_price, price, research_date, date_type, business \n"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nimport pandas as pd\nimport numpy as np\nproduct_month_price_pdf \u003d pd.read_csv(\u0027/DB_data/product_month_price_table.csv\u0027)\nproduct_month_price_pdf\nproduct_month_price_sdf \u003d spark.read.csv(\u0027/DB_data/product_month_price_table.csv\u0027, inferSchema\u003dTrue, header\u003dTrue)\nproduct_month_price_sdf.show()\n# product_month_price_pdf.rename(columns\u003d{\u0027price\u0027:\u0027unit_price\u0027}, inplace\u003dTrue)\n# product_month_price_pdf.insert(2, \u0027price\u0027,np.nan)\n# product_month_price_pdf.rename(columns\u003d{\u0027business_type\u0027:\u0027business\u0027}, inplace\u003dTrue)\n# product_month_price_pdf \u003d product_month_price_pdf[[\u0027product_id\u0027,\u0027unit_price\u0027,\u0027price\u0027,\u0027research_date\u0027,\u0027date_type\u0027,\u0027business\u0027]] \n# product_month_price_pdf.to_csv(\u0027/DB_data/product_month_price_table.csv\u0027,header\u003dTrue, index\u003dFalse)"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nimport pandas as pd\nimport numpy as np\nproduct_month_price_pdf \u003d pd.read_csv(\u0027/DB_data/product_month_price_table.csv\u0027)\nproduct_month_price_pdf\n# product_month_price_pdf.rename(columns\u003d{\u0027unit_price1\u0027:\u0027unit_price\u0027}, inplace\u003dTrue)\n# product_month_price_pdf.drop(\u0027unit_price.1\u0027, axis\u003d1, inplace\u003dTrue)\n# product_month_price_pdf\n# product_month_price_pdf.insert(2, \u0027price\u0027,np.nan)\n# product_month_price_pdf.rename(columns\u003d{\u0027business_type\u0027:\u0027business\u0027}, inplace\u003dTrue)\n# product_month_price_pdf \u003d product_month_price_pdf[[\u0027product_id\u0027,\u0027unit_price\u0027,\u0027price\u0027,\u0027research_date\u0027,\u0027date_type\u0027,\u0027business\u0027]] \n# product_month_price_pdf.to_csv(\u0027/DB_data/product_month_price_table.csv\u0027,header\u003dTrue, index\u003dFalse)"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%MySQL\n"
    }
  ]
}