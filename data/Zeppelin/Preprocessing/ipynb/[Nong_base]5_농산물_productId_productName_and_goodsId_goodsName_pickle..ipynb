{
  "metadata": {
    "name": "[Nong_base]5_농산물_productId_productName_and_goodsId_goodsName_pickle",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# 1. DB에서 바꿔야 하는 것들\n+ ~~goods table 변경~~\n    + ~~name 변경~~ \n    + ~~product_id 변경하면서, productIdOld_productIdNew pickle 만들기~~\n+ ~~product table 변경~~\n    + ~~productIdOld_productIdNew로 product_id 변경~~\n    + ~~product name 변경~~\n    + ~~product measure 변경~~\n    + ~~productId_categoryId.pickle 생성~~\n\n+ pickle 최신화 (전체 상품들에 대해서)\n    + **goodsId_goodsName** -1\n    + **goodsName_goodsId** -1\n    + **goodsId_productId** -2\n    + **productId_goodsId** -2\n    + **goods_amount** -3  : 같이 확인하기\n    + **productId_productName** -4\n    + **productName_productId** -4\n    + **categoryId_categoryName** -5\n    + **categoryName_categoryId** -5\n\n+ product_price의 date_type \u003d \u0027m\u0027 경우의 것들 productId 바꿔주기\n    + productIdOld_productIdNew로 product_id 변경"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## [1] goodsId_goodsName \u0026 goodsName_goodsId 경우"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### (1) 기존 pickle과 새로운 pickle load"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# 기존 전체 pickle load\nimport pickle\nwith open(\u0027/DB_data/goodsName_goodsId_new.pickle\u0027,\u0027rb\u0027) as fr:\n    goodsName_goodsId_new \u003d  pickle.load(fr)\ngoodsName_goodsId_new"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# 농산물이 바뀐 새로운 pickle load\nimport pickle\nwith open(\u0027/nongsanmul_data/nong_goodsName_goodsId.pickle\u0027,\u0027rb\u0027) as fr:\n    nong_goodsName_goodsId \u003d  pickle.load(fr)\n\n\nwith open(\u0027/nongsanmul_data/nong_goodsName_goodsId.pickle\u0027,\u0027rb\u0027) as fr:\n    nong_goodsName_goodsId \u003d  pickle.load(fr)\nprint(nong_goodsName_goodsId)\nprint(nong_goodsId_goodsName)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### (2) 새로운 pickle 생성\n+ goodsName_goodsId_1001\n+ goodsId_goodsName_1001"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ngoodsName_goodsId_1001 \u003d {}\ngoodsId_goodsName_1001 \u003d {}\n\n# 1. 기존의 goodsId_goodsName_new 생성\ngoodsId_goodsName_new \u003d {}\nfor goodsName, goodsId in goodsName_goodsId_new.items():\n    goodsId_goodsName_new[goodsId] \u003d goodsName\ngoodsId_goodsName_new\n\n# 2. 새로운 것 생성 : goodsId_goodsName_1001\nfor goodsId in goodsId_goodsName_new:\n    # 농산물에 있다면, Name 바꾸기\n    if goodsId in nong_goodsId_goodsName:\n        goodsId_goodsName_1001[goodsId] \u003d nong_goodsId_goodsName[goodsId]\n    \n    elif goodsId \u003d\u003d 20011: # 참다래 국산 삭제\n        continue\n    \n    else:\n        goodsId_goodsName_1001[goodsId] \u003d goodsId_goodsName_new[goodsId]\n\n# 3. 새로운 것 생성 : goodsName_goodsId_1001\nfor goodsId, goodsName in goodsId_goodsName_1001.items():\n    goodsName_goodsId_1001[goodsName] \u003d goodsId\n\n\nprint(goodsId_goodsName_1001)\nprint(goodsName_goodsId_1001)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### (3) 새로운 pickle 저장"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nimport pickle\n\nwith open(\u0027/DB_data/[20221001]goodsId_goodsName.pickle\u0027,\u0027wb\u0027) as fw:\n    pickle.dump(goodsId_goodsName_1001, fw)\n\nwith open(\u0027/DB_data/[20221001]goodsName_goodsId.pickle\u0027,\u0027wb\u0027) as fw:\n    pickle.dump(goodsName_goodsId_1001, fw)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## [2] goodsId_productId_1001 \u0026 productId_goodsId_1001"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nimport pandas as pd\n# goods \u003d pd.read_csv(\u0027/DB_data/goods_table_1001.csv\u0027)\n# goods.to_csv(\u0027/DB_data/[20221001]goods_table.csv\u0027, index\u003dFalse, header\u003dTrue)\ngoods"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ngoodsId_productId_1001 \u003d {}\nproductId_goodsId_1001 \u003d {}\n\nfor i in range(len(goods)):\n    goodsId \u003d goods.loc[i,\u0027goods_id\u0027]\n    productId \u003d goods.loc[i,\u0027product_id\u0027]\n    \n    # 1. goodsId_productId_1001\n    goodsId_productId_1001[goodsId] \u003d productId\n    \n    # 2. productId_goodsId_1001\n    if productId in productId_goodsId_1001 :\n        productId_goodsId_1001[productId].append(goodsId)\n    else:\n        productId_goodsId_1001[productId] \u003d [goodsId]\n\nprint(goodsId_productId_1001)\nprint(productId_goodsId_1001)"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# 저장\nimport pickle\nwith open(\u0027/DB_data/[20221001]goodsId_productId.pickle\u0027, \u0027wb\u0027) as fw:\n    pickle.dump(goodsId_productId_1001, fw)\nwith open(\u0027/DB_data/[20221001]productId_goodsId.pickle\u0027, \u0027wb\u0027) as fw:\n    pickle.dump(productId_goodsId_1001, fw)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## [3] goods_amount.pickle"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### (1) 기존 pickle load"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nimport pickle\nwith open(\u0027/DB_data/goods_amount.pickle\u0027,\u0027rb\u0027) as fr:\n    old_goods_amount \u003d pickle.load(fr)\nold_goods_amount"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## [4] productId_productName \u0026 productName_productId\n\n\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nimport pandas as pd\nproduct  \u003d pd.read_csv(\u0027/DB_data/product_table_1001.csv\u0027)\nproduct.to_csv(\u0027/DB_data/[20221001]product_table.csv\u0027, header\u003dTrue, index\u003dFalse)\nproduct"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nproductId_productName \u003d {}\nproductName_productId \u003d {}\n\nfor i in range(len(product)):\n    productId \u003d product.loc[i,\u0027product_id\u0027]\n    productName \u003d product.loc[i,\u0027name\u0027]\n    \n    # 1. productId_productName\n    productId_productName[productId] \u003d productName\n    \n    # 2. productName_productId\n    productName_productId[productName] \u003d productId\n\nprint(productId_productName)\nprint(productName_productId)"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# 저장\nimport pickle\nwith open(\u0027/DB_data/[20221001]productId_productName.pickle\u0027, \u0027wb\u0027) as fw:\n    pickle.dump(productId_productName,fw)\nwith open(\u0027/DB_data/[20221001]productName_productId.pickle\u0027, \u0027wb\u0027) as fw:\n    pickle.dump(productName_productId,fw)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## [5] categoryId_categoryName, categoryName_categoryId\n"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "\n%pyspark\n\nurl \u003d \"jdbc:mysql://j7d108.p.ssafy.io:3306/gamulgamul_test3?useSSL\u003dfalse\u0026characterEncoding\u003dUTF-8\u0026useUnicode\u003dtrue\u0026allowPublicKeyRetrieval\u003dtrue\u0026serverTimezone\u003dAsia/Seoul\"\n\n# 1. table 가져오기 : dbtable에 table 이름\ncategory_sdf \u003d spark.read.format(\"jdbc\")\\\n    .option(\"url\", url ) \\\n    .option(\"driver\", \"com.mysql.cj.jdbc.Driver\")\\\n    .option(\"dbtable\", \"category\") \\\n    .option(\"user\", \"ji\")\\\n    .option(\"password\", mysql_password).load()\n    \ncategory \u003d category_sdf.toPandas()\ncategory.to_csv(\u0027/DB_data/[20221001]category_table.csv\u0027,header\u003dTrue, index\u003dFalse)\ncategory"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ncategoryId_categoryName \u003d {}\ncategoryName_categoryId \u003d {}\n\n# 생성\nfor i in range(len(category)):\n    categoryId \u003d category.loc[i,\u0027category_id\u0027]\n    categoryName \u003d category.loc[i,\u0027name\u0027]\n    \n    # categoryId_categoryName\n    categoryId_categoryName[categoryId] \u003d categoryName\n\n    # categoryName_categoryId\n    categoryName_categoryId[categoryName] \u003d categoryId\nprint(categoryId_categoryName)\nprint(categoryName_categoryId)"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# 저장\nimport pickle\nwith open(\u0027/DB_data/[20221001]categoryId_categoryName.pickle\u0027, \u0027wb\u0027) as fw:\n    pickle.dump(categoryId_categoryName,fw)\nwith open(\u0027/DB_data/[20221001]categoryName_categoryId.pickle\u0027, \u0027wb\u0027) as fw:\n    pickle.dump(categoryName_categoryId,fw)"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%MySQL\nselect * from category\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    }
  ]
}