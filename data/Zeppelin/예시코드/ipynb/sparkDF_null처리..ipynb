{
  "metadata": {
    "name": "sparkDF_null처리",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.conf\n\nSPARK_HOME  /usr/local/spark\nPYSPARK_PYTHON /usr/bin/python3\nspark.pyspark.python  /usr/bin/python3\n\n# set driver memory to 8g\nspark.driver.memory 8g\n\n# set executor number to be 3\nspark.executor.instances  3\n\n# set executor memory 4g\nspark.executor.memory  2g"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## [1] null 찾기 함수 차이\n### (1) isnan()\n+ nan(not a number)만 찾음\n+ 값 안들어간 csv를 Pandas로 읽은 후, sparkDF로 변형하면, isnan()으로 처리해줘야 함\n\n### (2) .isNull()\n+ null 만 찾음"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark.sql.functions import col, isnan\nsdf.filter(isnan(col(\u0027Age\u0027))).show()\nsdf.filter(~isnan(col(\u0027Age\u0027))).show()\n\nsdf.filter(col(\u0027Age\u0027).isNull()).show()\nsdf.filter(col(\u0027Age\u0027).isNotNull()).show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## [2] fillna : null 채우기\n"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# img column의 null을 \u0027-\u0027로 바꾸기\nsdf.fillna({\u0027img\u0027:\u0027-\u0027})"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# 평균으로 결측치 채우기\nfrom pyspark.ml.feature import Imputer\n\nimputer \u003d Imputer(\n    inputCols\u003d[\u0027Age\u0027,\u0027Experience\u0027, \u0027Salary\u0027],\n    outputCols\u003d[\u0027{}_imputed\u0027.format(c) for c in [\u0027Age\u0027,\u0027Experience\u0027, \u0027Salary\u0027]]\n).setStrategy(\"mean\")  # mean, median, mode\n\nimputer.fit(sdf).transform(sdf).show() "
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## [3] drop : null있는 행 버리기"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# 전체가 null 인 record 삭제\nsdf.dropna(how\u003d\u0027all\u0027).show()"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# row의 non-null 값 개수가 thresh 미만이면 삭제\nsdf.dropna(how\u003d\u0027any\u0027,thresh\u003d3).show()"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# subset에 해당하는 column에서만 결측치 처리\nsdf.dropna(how\u003d\u0027any\u0027, subset\u003d[\u0027Experience\u0027]).show()"
    }
  ]
}