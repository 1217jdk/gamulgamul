{
  "metadata": {
    "name": "sparkDF_union",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 2개의 SparkDF row 방향 합치기\n"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nsdf1 \u003d spark.read.csv(\u0027/nongsanmul_data/20211001.csv\u0027, inferSchema\u003dTrue, header\u003dTrue, encoding\u003d\u0027cp949\u0027)\nsdf2 \u003d spark.read.csv(\u0027/nongsanmul_data/20211101.csv\u0027, inferSchema\u003dTrue, header\u003dTrue, encoding\u003d\u0027cp949\u0027)\nsdf1.show()\nsdf2.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nsdf \u003d sdf1.union(sdf2)\nprint(sdf1.count())\nprint(sdf2.count())\nprint(sdf.count())"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### (2) 여러개 한번에 union하기"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom functools import reduce  # For Python 3.x\nfrom pyspark.sql import DataFrame\n\ndef unionAll(*dfs):\n    return reduce(DataFrame.unionAll, dfs)\n\n\nunionAll(sdf1, sdf1, sdf1).count()\n# unionAll(sdf1, sdf1, sdf1).show()"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    }
  ]
}