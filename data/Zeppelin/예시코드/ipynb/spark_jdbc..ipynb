{
  "metadata": {
    "name": "spark_jdbc",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### (1) 읽기\n"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\r\n\r\nurl \u003d \"jdbc:mysql://j7d108.p.ssafy.io:3306/gamulgamul_test3?useSSL\u003dfalse\u0026characterEncoding\u003dUTF-8\u0026useUnicode\u003dtrue\u0026allowPublicKeyRetrieval\u003dtrue\u0026serverTimezone\u003dAsia/Seoul\"\r\n\r\n# 1. table 가져오기 : dbtable에 table 이름\r\ncategory_sdf \u003d spark.read.format(\"jdbc\")\\\r\n    .option(\"url\", url ) \\\r\n    .option(\"driver\", \"com.mysql.cj.jdbc.Driver\")\\\r\n    .option(\"dbtable\", \"category\") \\\r\n    .option(\"user\", \"ji\")\\\r\n    .option(\"password\", mysql_password).load()\r\n    \r\n\r\n# 2. 쿼리로 가져오기! dbtable 대신 query 옵션 이용하면 됨!\r\ncategory_sdf \u003d spark.read.format(\"jdbc\")\\\r\n    .option(\"url\", url) \\\r\n    .option(\"driver\", \"com.mysql.cj.jdbc.Driver\")\\\r\n    .option(\"query\", \"select * from category where category_id\u003d0\")\\\r\n    .option(\"user\", \"ji\")\\\r\n    .option(\"password\", mysql_password).load()\r\ncategory_sdf.show() "
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### (2) 쓰기\n"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark.sql.functions import col\n# 4 - 2 : MySQL에 데이터 넣기 : 성공 (https://ko.n4zc.com/article/sa4p4m6e.html)\nprop \u003d {\"user\": \"ji\", \"password\": mysql_password, \"driver\": \"com.mysql.cj.jdbc.Driver\"} \nurl \u003d \"jdbc:mysql://j7d108.p.ssafy.io:3306/gamulgamul_test3?useSSL\u003dfalse\u0026characterEncoding\u003dUTF-8\u0026useUnicode\u003dtrue\u0026allowPublicKeyRetrieval\u003dtrue\u0026serverTimezone\u003dAsia/Seoul\"\n\n# category_sdf \u003d spark.read.csv(\u0027/DB_data/category_table.csv\u0027, header\u003dTrue, inferSchema\u003dTrue)\ncategory_sdf.select(\"category_id\",\"img\",\"name\").write.jdbc(\\\n    url\u003d url,\\\n    table \u003d \"category\",\\\n    mode\u003d\"append\",\\\n    properties\u003dprop)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### (3) update\n+ pyspark에서 MySQL paragraph로 post 요청 보내서 진행\n+ 요청 주소\n    + url \u003d \"http://3.36.106.26:8081/api/notebook/run/notebookID/paragraphID\" \n+ 파라미터\n    + json\u003d{\"params\": {\"cheap_url\": cheap_url, \"goods_id\": str(goods_id)}} "
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%MySQL\nUPDATE goods SET cheap_url \u003d \u0027${cheap_url}\u0027 WHERE goods_id \u003d CAST(\u0027${goods_id}\u0027 AS UNSIGNED);"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# automatical run paragraph to update sql goods table cheap_url column\nimport requests\nurl \u003d \"http://3.36.106.26:8081/api/notebook/run/2HD9NBXQX/paragraph_1664176216600_9409959\"\n\nfor name in g_name:\n    goods_id, cheap_url \u003d get_goods_cheap_url(name)\n    req \u003d requests.post(url, json\u003d{\"params\": {\"cheap_url\": cheap_url, \"goods_id\": str(goods_id)}})"
    }
  ]
}