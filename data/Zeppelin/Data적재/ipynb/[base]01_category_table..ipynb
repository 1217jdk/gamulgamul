{
  "metadata": {
    "name": "[base]01_category_table",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.conf\n\nSPARK_HOME  /usr/local/spark\nPYSPARK_PYTHON /usr/bin/python3\nspark.pyspark.python  /usr/bin/python3\n\n# set driver memory to 8g\nspark.driver.memory 6g\n\n# set executor number to be 3\nspark.executor.instances  4\n\n# set executor memory 4g\nspark.executor.memory  2g\n"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark.sql.functions import col\n# MySQL에 데이터 넣기 : 성공 (https://ko.n4zc.com/article/sa4p4m6e.html)\nprop \u003d {\"user\": \"ji\", \"password\": \"cvgkbhs234r#8tdx\", \"driver\": \"com.mysql.cj.jdbc.Driver\"} \nurl \u003d \"jdbc:mysql://j7d108.p.ssafy.io:3306/gamulgamul_test3?useSSL\u003dfalse\u0026characterEncoding\u003dUTF-8\u0026useUnicode\u003dtrue\u0026allowPublicKeyRetrieval\u003dtrue\u0026serverTimezone\u003dAsia/Seoul\"\n\n# 저장소에서 읽기\ncategory_sdf \u003d spark.read.csv(\u0027/DB_data/category_table.csv\u0027, header\u003dTrue, inferSchema\u003dTrue)\n\n# DB에 쓰기\ncategory_sdf.write.jdbc(\\\n    url\u003d url,\\\n    table \u003d \"category\",\\\n    mode\u003d\"append\",\\\n    properties\u003dprop)\n    "
    }
  ]
}