{
  "metadata": {
    "name": "[daily]04_api_file_DB(o)_API(o)_toDB",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": " # 1. DB(o) \u0026 API(o) 인, 데이터 넣기\n\n+ Null 처리 notebook 실행 후에 해당 notebook 실행\n"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.conf\n\nSPARK_HOME  /usr/local/spark\nPYSPARK_PYTHON /usr/bin/python3\nspark.pyspark.python  /usr/bin/python3\n\n# set driver memory to 8g\nspark.driver.memory 8g\n\n# set executor number to be 3\nspark.executor.instances  3\n\n# set executor memory 4g\nspark.executor.memory  2g\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### (1) 무결성 정도 데이터 가져오기 : (2)에 들어가있음"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nimport pandas as pd\nimport datetime\n# today \u003d ( datetime.datetime.now()  + datetime.timedelta(hours\u003d9) ).strftime(\"%Y-%m-%d\")\n# yesterday \u003d ( datetime.datetime.now() - datetime.timedelta(days\u003d1) + datetime.timedelta(hours\u003d9)).strftime(\"%Y-%m-%d\")\n\ntoday \u003d \u00272022-10-04\u0027 # 임시\nDBo_APIo_pdf \u003d pd.read_csv(f\u0027/api_data/integrity/{today}_DBo_APIo.csv\u0027)\nDBo_APIo_pdf\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### (2) goods_id_lst에 있는 것들만, DB의 goods_price table에 넣기"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# 2HFEP1SVV\n# paragraph_1664852338546_377176280\n# http://3.36.106.26:8081/api/notebook/run/2HFEP1SVV/paragraph_1664852338546_377176280\n\n\nfrom pyspark.sql.functions import col, lit\nimport pandas as pd\nimport datetime\n\n\n\n# 1. 날짜 설정 /  DB에 넣을 GOODS_ID 정보 Load(무결성 정도 데이터 가져오기)\n\ntoday \u003d ( datetime.datetime.now()  + datetime.timedelta(hours\u003d9) ).strftime(\"%Y-%m-%d\")\nyesterday \u003d ( datetime.datetime.now() - datetime.timedelta(days\u003d1) + datetime.timedelta(hours\u003d9)).strftime(\"%Y-%m-%d\")\n# today \u003d \u00272022-10-05\u0027  \n# yesterday \u003d \u00272022-10-04\u0027\nDBo_APIo_pdf \u003d pd.read_csv(f\u0027/api_data/integrity/{today}_DBo_APIo.csv\u0027)\nDBo_APIo_pdf\n\n\n\n# 2. goods_id_lst 가져오기\n## 함수정의 : string을 list로 바꾸기 : \u0027[1,2,3]\u0027 -\u003e [1,2,3]\ndef str_to_tuple(str_lst):\n    if str_lst !\u003d\u0027[]\u0027: # 빈 것이 아닌 경우\n        result \u003d str_lst.replace(\u0027[\u0027,\u0027\u0027)\n        result \u003d result.replace(\u0027]\u0027,\u0027\u0027)\n        result \u003d result.strip().split(\u0027,\u0027)\n        result \u003d [s.strip() for s in result]\n        # print(result)\n        result \u003d sorted(list(map(int,result)))\n    else:\n        result \u003d []\n    return result\n\n# 3. DB 정보 및 Load \nurl \u003d \"jdbc:mysql://j7d108.p.ssafy.io:3306/gamulgamul_test3?useSSL\u003dfalse\u0026characterEncoding\u003dUTF-8\u0026useUnicode\u003dtrue\u0026allowPublicKeyRetrieval\u003dtrue\u0026serverTimezone\u003dAsia/Seoul\"\nprop \u003d {\"user\": \"ji\", \"password\": mysql_password, \"driver\": \"com.mysql.cj.jdbc.Driver\"} \n\n\n\n## 3-1 goods_price(사용하진 않는 부분.)\ngoods_sdf \u003d spark.read.format(\"jdbc\")\\\n    .option(\"url\", url) \\\n    .option(\"driver\", \"com.mysql.cj.jdbc.Driver\")\\\n    .option(\"query\", f\"select goods_id from goods\") \\\n    .option(\"user\", \"ji\")\\\n    .option(\"password\", mysql_password).load()\n\nmart_goods_price_sdf \u003d spark.read.format(\"jdbc\")\\\n    .option(\"url\", url) \\\n    .option(\"driver\", \"com.mysql.cj.jdbc.Driver\")\\\n    .option(\"query\", f\"select goods_id, unit_price, price, research_date, business from goods_price where research_date\u003d\u0027{yesterday}\u0027 and business\u003d\u0027m\u0027 \") \\\n    .option(\"user\", \"ji\")\\\n    .option(\"password\", mysql_password).load()\n\n    \nonline_goods_price_sdf \u003d spark.read.format(\"jdbc\")\\\n    .option(\"url\", url) \\\n    .option(\"driver\", \"com.mysql.cj.jdbc.Driver\")\\\n    .option(\"query\", f\"select goods_id, unit_price, price, research_date, business from goods_price where research_date\u003d\u0027{yesterday}\u0027 and business\u003d\u0027o\u0027 \") \\\n    .option(\"user\", \"ji\")\\\n    .option(\"password\", mysql_password).load()\n\nprint(f\u0027전체 {goods_sdf.count()}개 상품 중, 전날 마트 : {mart_goods_price_sdf.count()} 개, 전날 온라인 :  {online_goods_price_sdf.count()} 개가 들어있음\u0027)\n\n\n\n## 3-2. 추가할 csv Load\ntp_goods_price_m_sdf \u003d spark.read.csv(f\u0027/api_data/tp_goods_price_m_{today}.csv\u0027, header\u003dTrue, inferSchema\u003dTrue)\nnong_goods_price_m_sdf \u003d spark.read.csv(f\u0027/api_data/nong_goods_price_m_{today}.csv\u0027 , header\u003dTrue, inferSchema\u003dTrue)\n\nmart_goods_price_sdf \u003d tp_goods_price_m_sdf.union(nong_goods_price_m_sdf)\nonline_goods_price_sdf \u003d spark.read.csv(f\u0027/api_data/goods_price_o_{today}.csv\u0027 , header\u003dTrue, inferSchema\u003dTrue)\n\n# mart_goods_price_sdf.show(n\u003d1000)\n# online_goods_price_sdf.show(n\u003d1000)\n\n\n## 3-2 lst 가져오기 \u0026  / DB에 넣기(주의!!!!!!!!!!!)\nm_selected_DBo_APIo_pdf \u003d DBo_APIo_pdf[(DBo_APIo_pdf[\u0027business\u0027]\u003d\u003d\u0027m\u0027 )\u0026 ( DBo_APIo_pdf[\u0027research_date\u0027]\u003d\u003dtoday)].reset_index()\n\nif len(m_selected_DBo_APIo_pdf): # m_selected_DBo_APIo_pdf 존재할 때만, DB에 적재\n    m_goods_lst \u003d m_selected_DBo_APIo_pdf.loc[0,\u0027goods_id_lst\u0027]\n    m_goods_lst \u003d str_to_tuple(m_goods_lst)\n    \n    ### 1. 값 넣을 것들만 filter\n    mart_goods_price_sdf   \u003d mart_goods_price_sdf.filter(col(\u0027goods_id\u0027).isin(m_goods_lst))\n    \n    ### 2. research_date 바꿔주기\n    mart_goods_price_sdf \u003d mart_goods_price_sdf.withColumn(\u0027research_date\u0027, lit(f\u0027{today}\u0027))\n    mart_goods_price_sdf.orderBy(\u0027goods_id\u0027).show(n\u003d1000)\n    # mart_goods_price_sdf.filter(col(\u0027goods_id\u0027)\u003e\u003d20000).show(n\u003d100)\n    \n    ### 3. 저장요청보내기         %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n    mart_goods_price_sdf.write.jdbc(url\u003d url,  table \u003d \"goods_price\", mode\u003d\"append\",properties\u003dprop)\n    # mart_goods_price_sdf.filter(col(\u0027goods_id\u0027)\u003e\u003d20000).write.jdbc(url\u003d url,  table \u003d \"goods_price\", mode\u003d\"append\",properties\u003dprop)\n    print(\u0027mart_data 가 들어감\u0027)\n\nelse:\n    print(\u0027DB에 넣을 mart data가 없습니다\u0027)\n\n\no_selected_DBo_APIo_pdf \u003d DBo_APIo_pdf[(DBo_APIo_pdf[\u0027business\u0027]\u003d\u003d\u0027o\u0027 )\u0026 ( DBo_APIo_pdf[\u0027research_date\u0027]\u003d\u003dtoday)].reset_index()\nif len(o_selected_DBo_APIo_pdf):  # o_selected_DBo_APIo_pdf 가 있는 경우에만 추가\n    o_goods_lst \u003d o_selected_DBo_APIo_pdf.loc[0,\u0027goods_id_lst\u0027]\n    o_goods_lst \u003d str_to_tuple(o_goods_lst)\n    \n    ## 2-2. 값 넣을 것들만 filter\n    online_goods_price_sdf \u003d online_goods_price_sdf.filter(col(\u0027goods_id\u0027).isin(o_goods_lst))\n    ## 2-3. research_date 바꿔주기\n    online_goods_price_sdf \u003d online_goods_price_sdf.withColumn(\u0027research_date\u0027, lit(f\u0027{today}\u0027))\n    online_goods_price_sdf.orderBy(\u0027goods_id\u0027).show(n\u003d1000)\n    \n    ### 3. 저장요청보내기         %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n    online_goods_price_sdf.write.jdbc( url\u003d url, table \u003d \"goods_price\", mode\u003d\"append\", properties\u003dprop)\n    print(\u0027online_data 가 들어감\u0027)\n    \nelse:\n    print(\u0027DB에 넣을 online data가 없습니다\u0027)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nonline_goods_price_sdf.orderBy(\u0027goods_id\u0027).show(n\u003d500)"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\no_goods_lst"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    }
  ]
}